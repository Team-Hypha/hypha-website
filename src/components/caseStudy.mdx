export default ({ children }) => <div className="case-study">{children}</div>

# Hypha Case Study

## Introduction

Many applications today use distributed architectures. Distributed systems have the benefit of making systems scalable, reliable and maintainable. But with more components it becomes increasingly difficult to determine where a problem arose because we lack visibility into how requests propagate through the system.

In a monolithic system, a typical approach to debugging problems is to look at the logs when something goes wrong. Logs give a detailed view of individual events, but they only have insight into one component at a time, not the whole system. \[DTIP] It’s challenging to see how the logs produced from one component relate to other components.

One approach to addressing the lack of context between components is to use distributed tracing. Tracing gives a high-level overview of how requests move through a distributed system, and it provides more context for how components in the system interact. However, this still doesn’t address the challenge of connecting logs produced in one component to related logs produced in other components.

Hypha addresses this challenge by providing a telemetry pipeline that correlates trace data with log data. Hypha’s UI makes it possible to seamlessly move between the high-level view of traces and the detailed event-level view of logs, giving developers the context they need to debug effectively in a distributed system.

### Observability Concepts

In the field of software engineering, there are many interpretations and approaches to the concept of observability. For consistency, we’re going to use the following definition: “a measure of how well internal states of a system can be inferred from knowledge of its external outputs.”

At a fundamental level, software systems boil down to inputs and outputs. The computer takes an input, runs the software, and produces an output. The output gives insight into how the system is functioning, but what if the output is incorrect or the system is unresponsive? How can we inspect the internal state to solve the issue?

For a system to be properly observable, it needs to output enough data that its internal state can be inferred from the outside. The data it outputs for this purpose is called**_telemetry_**. Code that is written to output telemetry is called**_instrumentation._** Any developer who has written a \`console.log\` statement to debug their code has instrumented their code to output telemetry data.

There are different types of telemetry, with the three most common being logs, metrics, and traces. These are often called the “Three Pillars” of observability. Hypha focuses primarily on logs and traces, which will be explored in more detail in the following scenario about a fictional startup.

**Metrics**

Metrics are a numerical measure that provides a snapshot of some aspect of a system’s behavior during a certain period of time. For instance, this could be requests per second, CPU or memory usage over time, or errors per minute. This data is usually visualized in a dashboard, providing a quick overview of a system’s health.

**Logs**

Logs are human-readable lines of text that provide specific detail about an event. They’re great for providing fine-grained, detailed information originating from a single application.

<img src="https://lh4.googleusercontent.com/kDbYaZvFJOwcAKUir8gs5HZjVb1IhE3-3Uov3he_K5t13RtZd5MHSJr_ktW3WrE6C884cbXBzK3HAoq_ySX42WLmK6fzOkseD470ehAU-2Vul3kg2qQCgN1iqd2ZCxu91jOacKho" />

Unstructured Log

**Traces**

- What are they in general
- Spans
- Time & Relationships

Traces provide a higher-level view when compared to logs. They show the path a request took through a system, and they provide information about each unit of work along that path.

<img src="/labra/Labra-Distributed.jpg" />

<img src="/labra/Labra_Trace.jpg" />

For example, on the right we see a trace visualized as a waterfall chart.

A trace is composed of spans, each of which represents a unit of work and the amount of time taken to complete that work. The trace starts with the root span where the request entered the system (here the API gateway). From there, the waterfall chart illustrates the relationships between subsequent requests. Traces provide context for how a given request travels through the system and for how different components in the system interact.

## Introducing Labra Cadabra

Labra Cadabra is a small health-tech startup company. They provide a platform for patients and diagnostic labs to handle payment, scheduling, and viewing test results.

### Using Logs Locally

Amy is a senior engineer at Labra Cadabra. Here she is working on her local computer, adding a feature to their application.When she tests the new code, she finds that a certain input is producing a different output than she expects.Looking only at the output, it’s not obvious what’s wrong.

She can use logging to understand why the code is behaving this way. She writes a few lines of code to output the extra information she needs. In other words, she instruments her code to emit logs. She can use these extra details to pinpoint and fix the issue.

<img src="/storyline/Storyline_Using-Logs.GIF" />

Now that Amy has fixed the issue, her code is ready to deploy to Labra Cadabra’s production server. What will this mean for her ability to understand her code through logging?

### Using Logs Locally vs. In Production

During development, Amy can add some logging whenever she would like to output some information. She can then run the program again and see the output within a matter of seconds or minutes. Logs are often not saved since she is using them right away. Additionally, she doesn’t need to put very much thought into the structure and format of the logs; since she just wrote the code that will output the logs, she has the context she needs to understand the information they contain.

In production, several things change. If an issue arises, any member of the team needs to be able access and understand the recent logs. Logs must be persisted and be output in a useful and consistent format. In addition, the time it takes to get new logging into the code has significantly increased. Amy can no longer decide she would like to log the value of a certain variable and see it on her screen a few seconds later. It is important that they make sure their code is well instrumented before being deployed, otherwise they might find they are missing key details needed to understand the state of their application.

Amy has instrumented her code to output logs to a file on the server. When she receives a report of an error, Amy SSHs into the server and reads the log file. She can use Unix tools like \`grep\` to search the logs or \`tail\` to view the most recent logs. The Labra Cadabra team invests time and effort in making sure they are logging useful information. This investment allows them to effectively debug issues when they arise.

<img src="storyline/Storyline_Using-Logs-Production.GIF" />

This is also true of the structure and formatting of the logs. Locally, a developer doesn’t necessarily need a lot of structure to understand the logs they just wrote. But to be useful in production, logs need to have consistent structure and formatting so that anyone can read and understand them.

### Microservices

As Labra Cadabra grows, they make the decision to move to a microservice architecture. This has several benefits:

- Services can scale individually with demand, allowing better resource utilization
- Each team can focus on a service, meaning they can be responsible for a smaller codebase
- Teams are more independent meaning they can make technology choices that are best for their service

Of course, like everything else in software engineering, this has come with tradeoffs.

<img src="/labra/Labra-Distributed.jpg" />

#### Log Aggregation

Previously, Amy was able to SSH into one server to find the logs she needed. Now that services are separated, logs are produced in many nodes and it is tedious to SSH into each service. This slows down debugging. One solution to this is to aggregate logs in one place. This allows the team to find what they need in one location, making debugging quicker.

<img src="/labra/Labra-Distributed_Logs.GIF" />

There are several ways to do this. They can change their logging instrumentation to send logs over the network to a central storage solution, or they can install a separate agent program that scrapes and ships logs from the log files they’re already using.

This will solve the issue of having logs distributed across many nodes, but there’s another challenge they face as the company grows. With the distributed nature of the new system it becomes more difficult to see how the execution of a single request flows through the different services.

#### Distributed Tracing

\*\*\* need some better visuals with animation - showing that there are many different possible routes through the system. This whole section needs a fair amount of reworking for prose

<img src="https://lh5.googleusercontent.com/HH0wXUckZ1IRL6RJKLnjax7bYvboUwMJEaFgJTI-cjvemXFUe1nzFqahnoYYY7sndhYtgmlnTDLGZQM7fM7zDVTlXg_84gstGvADwn3PcEChvzLXvxVTLsrSOzEtaRXrwU4eGz5H" />

If there is a problem with a request, how does an engineer go about determining which service is responsible for the problem? Without context for how a request travels through the system, it can be difficult to find the source of the issue. This is especially difficult for new hires who are still learning about the system. Amy and other senior engineers have enough experience and historical context that they often have a good idea of where to look when an issue arises. But even for senior engineers, this will continue to become more difficult as the system’s complexity grows. They want a solution that will help everyone get context quickly.

Fortunately, Amy and her team aren’t the first to encounter this challenge. Distributed tracing was developed to address this lack of context by providing information about the execution path of a request. The visual summary of the timing and flow of a request provides a quick and intuitive way to understand how a single request moved through the system and how components are connected. This makes it easier to quickly identify where problems are occurring.

For instance, if Amy gets complaints about a delayed request to the cart service, she could browse traces that touch the cart service. She takes a look and most of the recent traces look something like the waterfall on the left. But she then finds a few requests that look like the diagram on the right. Immediately she can see that the last call to the lab tests service is a lot longer than the others. While the trace data doesn’t tell her exactly why this is occurring, she is able to quickly identify that there may be a problem causing the lab tests service to respond slowly in this case.

_&lt; do we need the other example with repeated requests? - maybe incorporate the idea into the previous example >_

#### Using Logs and Traces Together

Logs contain details about specific events, and traces contain a high-level picture of the execution path. These two types of telemetry complement each other well, but there are challenges to consider in order to use them together effectively.

Going between logs and traces presents some interesting challenges. Consider the scenario where Amy is investigating a user-reported error that is occurring in the lab test service. She’s found a log that shows the error. This gives her details but she needs more context about the request that caused the error.

<img src="https://lh6.googleusercontent.com/4hkzV0RuWx18MC7_ferEg6Gy6D_2la049cMUipfp99p4bQlVovSWpAphm9ImsDdmiSocfM-ENq4BrJK7A_agDxtObQXUo8k4mBJOVX9PeCfyN-sAOVbJmuo6wmzQ2V9OYbNZO7Kv" />

This is a great use case for traces. The question is, how does Amy find the trace for the request that created this specific log? To demonstrate how she might do this, here is what she would need to do using the UI of a popular open-source tracing tool, Jaeger.

<img src="https://lh4.googleusercontent.com/qTYZpTZCQMnuirjZrI9Ta-FG_q0i60q0A_tjdeHWH0IyBQ7I7r1X2OZtQ7GpM6Gt6dbFSIzZx3MhnC5UZmBQv6Fw2y5dA6SVhBC84AktKuWU_wbuGh_40Pf0XMDWu1bsxyHFUj44" />

The log has a timestamp, so she can set a custom time range to capture traces that are around that time. She also knows what service it came from, so she can search only traces that contain that service. She knows it was a 500 error, so she can filter by error code.

Depending on how busy that service is and how many 500 errors occurred during the time range, Amy might have narrowed down the number of traces to a manageable amount. But she might also need to keep refining her search if there are too many traces. This process is tedious, especially if Amy is trying to do this in the middle of a high-stress incident.

Assuming that Amy has found the trace she is looking for, she would like to be able to take a look at the specific logs associated with this single request in the other services it touched. In distributed systems, the root cause of an error in one service might be an event that occurred in a different service. That service may not have even thrown an error, but there could be important details in the logs from that service. So how does she get from the trace to the logs?

&lt;visuals placeholder>

<img src="https://lh5.googleusercontent.com/_d95rh91kEVMecUb_84_0ZVLwh7-6Tv8_fmriImS_p72atiTl7etkopkLeFW-tGn--n_ujldS3ZNJ2wdFIeNDWcPrdLl2ihAmFqUbsv7w3Yz924eHlz9lstBFrAD9IbMenFDApjt" />

<img src="https://lh4.googleusercontent.com/ct0iS0R31l8EjCT_eN4_Esj-b4IC4_DbCmoLkzLRMlAIY7M090q5PDQtCOccGnI1dzE70MxwcHZeWUCSndVhB9w81KfKpf3tNVTU8mTZ-DZHsmqz-9G021JKRaLHvg2-AmKNljps" />

She can search the logs for that service and filter the timestamps based on when the span occurred. But there might be dozens or hundreds of logs in that timeframe. This can make looking for the ones they want “like looking for a needle in a stack of needles” \[TY]. Logs will all look the same and there is often not an easy way to differentiate them.

There are a number of ad hoc ways to correlate logs and traces but the core problem is that there is rarely one single piece of data that is present in every log and every trace that Amy and her team can predictably use to correlate.

Amy and her team would like to have a way to seamlessly connect their log data and their trace data. They need a single context or unique identifier that gets attached to every log and trace as it is created and that will travel with a request as it moves through their system. This will make it easier to move between associated logs and traces, speeding up debugging.

Amy and her team determine that they would like to implement log aggregation and distributed tracing. In addition, they would like their logs and traces to be correlated with each other.In order to achieve their desired level of observability, they will need to provision or build a telemetry pipeline that can generate and handle this data.

## Telemetry Pipeline

<img src="/pipeline/Hypha-Pipeline-Icons.jpg" />

Let’s get an overview of the major stages that compose a telemetry pipeline so that we have a better idea of what’s involved before moving forward. We’re going to use the model outlined by Jamie Riedesel in her book*Software Telemetry*, \[ST] which describes three major phases: Emitting, shipping, and presentation.

### Emit

The emitting stage iswhere we instrument the code to generate and emit the telemetry data that will flow through the rest of the system

For example, to generate traces, one approach is to use an SDK from a framework like Open Telemetry. When a new request hits the system, this code is responsible for generating the data that’s needed to start tracing the journey of that request through the system. For application logs, a common approach is to use a logging library such as Winston that generates and emits logs in a consistent, customizable format.

The emitting stage is also where we prepare the telemetry data for shipping. This preparation may include formatting or enriching the data with additional information. So, the approaches taken in the emitting stage will determine the initial format and content of the telemetry data before it enters the next stage of the pipeline.

### Ship

The shipping stage is concerned with collecting the data that’s been emitted, processing that data as needed, and storing it so that it can be used effectively in presentation.

Depending on the use case, this might also be where telemetry data is transformed or enriched to be more useful. For example, we might want to parse data into a format that’s better suited for the database we’re using, or we might add contextual information that makes our data more helpful later on. This kind of enrichment is also sometimes done in the emitting stage, and this is just one of the variations in implementation that can exist.

There are many other considerations in this stage, ranging from the type of storage we use, to how we aggregate and export our data, to how we process our data and why, and all of these choices are going to have a big impact on what we can do next.

### Present

The presentation stage is focused on querying data from our storage and transforming that data in a useful way. This usually means visualizing data as charts, tables, and graphs, and providing a user interface to interact with the telemetry data produced and stored by the earlier stages of the system.

This is the stage that users of the pipeline are going to interact with and see the most. For example, this is where a developer would actually go to investigate and debug an issue.

Later on, we will take a deeper dive into each of these sections as we discuss the approaches we took with Hypha. For now, let’s pick back up with Amy and her team as they investigate some of the existing solutions in this domain.

## Existing Solutions

### SaaS

One place they might start is with SaaS solutions, or software as a service sold by third-party vendors.

<img src="https://lh3.googleusercontent.com/cZkZz6TtmfnasIsm_SBb0Bf6ee7Wk3vVTPj652gChs-AygEw7PE2ffklIqTQpv07928P-xfsHuJEF2TSvjL4cqUDfNDO-Z2-x-H30z_Q8txFkAK-gr4gJSjFshbQbjJSTjF0dcdW" />

One of the main advantages of SaaS solutions is that there’s very little setup required to use them. In most cases, a developer just has to copy instrumentation code into their applications, and the vendor takes care of all the other stages in the pipeline. They also manage that pipeline, so the developer doesn’t have to worry about deploying, scaling, or maintaining that infrastructure. Finally, these solutions come with feature-rich UIs that can handle many data types and are highly customizable.

However, these benefits come with some drawbacks. First, by sending their data to a third party, Labra Cadabra would no longer have sole ownership of that data. As a healthcare company that needs to protect the privacy of their users, this is a pretty big drawback to consider.

Another difficulty with using SaaS solutions is that the huge feature sets can actually be a double-edged sword. While very powerful, they can also be overwhelming and difficult to learn, and Amy and her team would like to start using telemetry data without such a steep learning curve.

### DIY

Another approach they might look at is to use open source telemetry tools, which they could connect together to create a DIY solution.

<img src="https://lh6.googleusercontent.com/n_S7s_jFb3FbhljSg73q0CJfXPcf3PwiPIvPFeLTfGchXzqfCvTY898xtuci4LUlvHmgyQGUKdgf2hM7-IsXO7loOQJyNuixnpTS70CjwvSqvQsbK3RkWthIPFMd56Q_ZNH6FFFQ" />

One of the major appeals of this approach is that by using open source tools, Labra Cadabra would have control over their telemetry pipeline. They could customize it to fit their needs, and they could update and expand their pipeline as needed. Labra Cadabra would also retain data ownership, since they would own the pipeline itself.

But this also comes at a cost. The biggest challenge with this approach is the sheer amount of time and effort required to build a pipeline this way. The above logos represent just a portion of the tools that can be used for telemetry, and this a constantly evolving field; it can be difficult to stay up to date on what tools are best suited for certain use cases.

Amy and her team would need to invest a huge amount of time and effort in researching these tools. First, they would need to learn which tools fit their use case for each stage of the pipeline. Then, they would need to determine which of these tools are compatible. Once they’ve decided which tools to use, they would need to configure them to match their use case and make sure the tools integrate effectively.

<img src="https://lh4.googleusercontent.com/WZXkuZu09_MygzOLKe7-rmELyWbzWFNBcxQypfeIyoO3_0U9lTvtcE3Cb6Nwpv2ZtltI4ZcEnRFQMnwxSTCqoSe9eGcxXV08RmxfdmGVNR8xyOrmSqv2IN7TeiX9tuNOxSL11Nk8" />

To review, SaaS solutions are managed and easy to set up, but the team wouldn’t have control over the code, and they wouldn’t have the data ownership they want. DIY gives them more control and ownership, but that control comes at a heavy labor cost. They’d really like to have another way, and this is where Hypha fits in.

## Introduce Hypha

<img src="https://lh4.googleusercontent.com/2fkerhKKiAMaYQNsN1_Rmj610cQfn-5tlyS4rIn1xMsmyUNCcWZ7tAzPzi4qXwfdMtEyD5GeVvQzYWWmdBTOK0qQYtF1n5i0EPWrw91FHYDzzmiy7NVMDlHG4v9tzcFvXdyr62eU" />

Hypha was designed to provide the easy setup of a SaaS solution, along with the data ownership and control of a DIY approach. Hypha provides a ready-made telemetry pipeline, automates setup and deployment processes, and provides a UI that’s easy to learn and start using.

Our code is open-source, which means that teams have the option to customize and control the code if they want, but they don’t have to change anything to get started. Because Hypha is deployed to their own infrastructure, Labra Cadabra retains ownership of the data flowing through their pipeline.

There are a couple of trade-offs with using Hypha. One is that it’s not as feature-rich as some of the SaaS solutions. But this is actually suited for Amy and her team, who are looking for something easy to learn that they can get started with quickly. The other trade-off is that teams will have to manage the Hypha infrastructure themselves. This isn’t as convenient as a SaaS solution, but for Labra Cadabra, self-management is worth the effort in order to have control and data ownership.

### Demo Rework

Here’s the dashboard of the Hypha UI. It gives you an overview of your system health, showing you metrics like errors by service, logs by level or type, and also a list of logs that contain errors.

You can filter the data displayed on the dashboard by time range

and also by service name. If you want to take a closer look at one of the logs, you can click into it, and this will expand to show you more detail about that log.

This information includes the trace Id of the trace that’s associated with that log, which we use for correlation. To see this trace, you can click the Jaeger button, and you’ll be taken to the trace overview.

Here, we see a waterfall chart representing the trace that is associated with the log we were just looking at. If you want to investigate one of the spans in this trace, you can click into the span, and you’ll see expanded information about that span.

To see the logs that were produced during that span, you can click the logs for this span button, and a new frame will open up alongside, showing the logs specific to that span.

You can click into one of these logs to get more detail, and again you’ll get an expanded view of that log’s data. You can continue exploring your data in this way.

Going back and forth between logs and traces like this gives you a lot of flexibility and power in terms of how you investigate. We also have dedicated tabs that allow you to search your logs and traces individually.

Our goal was to provide a UI that was focused and easy to get started with. But there are also a lot of options for customization and expansion because our UI is powered by Grafana, and we include a link that allows you to access Grafana directly as well.

## Hypha’s Architecture

When we set out to build Hypha, we agreed it should meet the following Design Goals:

- have Drop in Functionality
- be Interoperable,
- be Easy to Use, and
- be Ready to Deploy

Recall the 3 phases of the telemetry pipeline from earlier: Emit, Ship, and Present. Within each phase of Hypha’s architecture, we chose specific tools and configurations.

### Emit

In the emit phase Hypha needs to generate traces, collect existing logs, create the correlation between logs and traces, and emit this data to the next phase of the pipeline. Because we wanted developers to be able to easily add Hypha to an existing application, we needed to accomplish these tasks while requiring minimal changes to existing code, and avoiding manual setup or complex configuration. Currently, Hypha can be used with NodeJS applications running on a server such as a virtual private server. We assume that the application is outputting logs to a local file on the server.

<img src="/architecture/Hypha-Instrumentation-Before.jpg" />

#### Generating Traces

The first consideration is for Hypha to generate traces. One option for generating tracing data is to manually instrument the application code. This involves using a tracing library to add instrumentation to the application code to create traces and spans. The developer must also specify the details and attributes to attach to spans and configure where they should be sent.

This approach allows the developer to have complete control over the data that will be included in each span. However, this takes time to learn and implement correctly. In addition, the process must be completed for each service they want to instrument. Asking this much of Hypha’s users went against our design goal of being a drop-in solution. We needed a solution for generating trace data with minimal set-up and changes to existing code.

To accomplish this we used an OpenTelemetry auto-instrumentation package. This package wraps JavaScript libraries and frameworks to generate traces without the need to change application code. One example is the network protocol library for HTTP. The instrumentation wraps the library, attaching HTTP request and response details to the current trace.

In addition to providing us with trace generation, the auto-instrumentation library also wraps several popular logging libraries and injects the current trace and span IDs into each log. Hypha later uses this injected context to correlate between logs and traces.

<img src="/architecture/Hypha-Instrumentation-Middle.jpg" />

#### Emitting Trace Data

Next, Hypha needs to send this telemetry data to the next phase of the pipeline. We could configure the tracing instrumentation to send this data directly, but there are advantages to offloading this responsibility from the main application. For instance, it makes it possible to perform processing on the traces, and batch the data, making for more efficient network usage. For this purpose we use the OpenTelemetry Collector. The collector runs as a separate binary on the same host as the instrumented application.

#### Handling Logs

There are several ways that Hypha could collect logs from the application. The most direct way would be to have the application send logs directly to an agent instead of to a file. However, this would require Hypha users to change their logging endpoint, violating our design goal of minimal changes to application code.

We decided to use an agent to read, process, and emit the data from existing log files. One option was to use a stand-alone program like FluentBit. This option would have added another binary application to run on the server, bringing greater complexity of installation and configuration. Instead, we chose to use the filelog receiver of the OpenTelemetry Collector. While this option is not as full-featured as some other solutions, we decided it made sense to keep the architecture simple and lightweight, using one collector to handle both logs and traces.

<img src="/architecture/Hypha-Instrumentation-After-multi.jpg" />

#### Sending Data

With instrumentation for logs and for traces in place, the Agent Collector can be configured to emit the telemetry data to Hypha’s backend. Since this process is needed across multiple services, Hypha provides a script to automate downloading and configuring the Tracing Libraries and Agent Collector.

### Ship

In the ship phase, Hypha needs to aggregate logs and traces from different services, it needs to handle data processing and enrichment, and it needs storage solutions that can handle high throughput. Hypha should also be interoperable and scalable in order to handle future changes.

Hypha uses the Open Telemetry Collector deployed as a central Gateway to handle aggregation, processing, and exporting. This approach provides many benefits. Open Telemetry is quickly becoming the gold standard for telemetry data, and it already integrates well with most observability tools. This helps make the backend interoperable, giving developers the flexibility to easily change tools if needed. For example, if a developer wants to export their trace data to a different tool, they can simply change a few lines of code in their Gateway configuration. Having a central gateway also makes it easier to configure data enrichment and processing from one place.

<img src="/architecture/Hypha-Collection-After.jpg" />

Following the Gateway Collector, Hypha needs storage solutions for both traces and logs. Production applications can produce a huge amount of telemetry data, so Hypha’s storage solutions should be able to handle high throughput. They need to write data quickly and should be able to scale. Hypha uses separate solutions for logs and traces in order to separate concerns and provide flexibility.

For logs, Hypha uses Grafana Loki. Loki is designed to index logs by key-value pairs called labels. Indexing this way makes Loki cheaper to operate than a full index and enables it to efficiently handle high log volumes. Additionally, we needed a solution that would work well with Grafana, which we chose to power Hypha’s UI. Because Loki is part of Grafana labs, it integrates well with Grafana and is well-suited to the way that Grafana handles telemetry linking.

For traces, Hypha uses Jaeger, an open source tracing tool with extensive community support. Initially, we explored Grafana Tempo as an option, since it integrates well with other Grafana Labs tools and is designed to link telemetry data. However, we chose Jaeger because it currently allows more extensive search options than Tempo, and we wanted to give developers more options to search their traces individually.

Both Loki and Jaeger are configured to run as scalable and platform-agnostic containerized services.

<img src="/architecture/Hypha-Store-After.jpg" />

### Present

In the Present phase, Hypha needs to query telemetry data from Loki and Jaeger, and it needs to visualize that data within a central UI. This UI should leverage Hypha’s correlated telemetry data to allow users to flow between related logs and traces. Hypha needs to accomplish this while also being easy to use: the UI shouldn’t require a steep learning curve.

One possibility was to create our own custom UI to handle querying and visualization. While this would have allowed a very focused experience, it would have meant significant effort and would not have been as feature rich as incorporating existing solutions.

Another option was to use Grafana, a powerful open-source tool that is capable of querying and visualizing data from many different sources. However, a downside to using Grafana is that its powerful feature set can be difficult to learn and start using. Because Grafana was designed to handle so many use cases, its UI can feel overwhelming.

To address this challenge, we developed the Hypha UI, which wraps Grafana in an intuitive, focused interface and provides customized dashboards. Developers can get started quickly without a steep learning curve, while maintaining access to Grafana’s powerful features.

Hypha leverages Grafana to proxy API calls to both Loki and Jaeger data sources. Grafana also provides feature-rich interfaces for querying both traces and logs. Lastly, and most importantly, Hypha configures Grafana to link related logs and traces within debugging flows.

<img src="/architecture/Hypha-Present-After.jpg" />

The Hypha UI completes the final stage of our end-to-end telemetry pipeline. This completes the final phase of Hypha. Together we now have an end to end telemetry pipeline.

<img src="/architecture/Hypha-Architecture.jpg" />

## Engineering Challenges

### Edit Correlating Logs With Traces

Recall that Amy and her team found that correlating logs with traces would provide many benefits to their debugging workflow. There are many approaches to consider around correlating these telemetry data types. Let’s take a closer look at some of those options, and then we’ll discuss why we chose the approach we did.

#### Types of Correlation

One approach is to correlate by the origin of telemetry. This means our common point of reference is the resource which emitted the logs and traces. For example, we could use the name of the service or the name of the VM.

We could also correlate by time of execution. This approach uses timestamps, correlating a log with a trace if they both occurred at the same moment in time.

A third approach is to correlate by execution context.

### Correlating Logs With Traces

#### Types of Correlation

##### Time

##### Origin of Telemetry

##### Execution Context

#### Span Events vs. Inject Span ID into Logs

Remember from earlier in the presentation Amy and her team found that correlating logs with traces would be really helpful in their debugging workflow. Let’s look at how we accomplished correlation in detail.

One option for correlating by the execution context involves replacing logs with span events. Span events are a form of structured logs appended directly to spans. Like logs, they’re for recording fine-grained events at a point in time.

<img src="https://lh6.googleusercontent.com/mb1vSm12qATof-k9fu2j88A_Je4MOcX1XDrsPxnsG0xF495uzozkeYU7r6cNpwY0CB8XYxEaq35qnkD1j5a35kp5xj5V5OtIty-iNaT-ucs1PX_rT9liT3IK-OA-Dm60SoW1fwIH" />

So, instead of being separate types of telemetry, logs are appended to spans as events and directly inherit the trace context. This option has a great advantage of simplifying the telemetry pipeline architecture: We only need to collect and store traces. However this would require users to refactor their logging instrumentation to append span events instead. Also for engineers used to working with logs, this would be a fundamental change in how their tools work, since logs are now embedded in the span data structure.

Another option correlating by execution context is to inject trace context into existing logs. This means accessing the trace ID and span ID and appending it as metadata to each log.

<img src="https://lh4.googleusercontent.com/TaOx2KR-7sqv15I8-UY-zm-WpW5yPDbCcMe5oXjHUUVqq4xPJUeJYRGKuXIwZYB7k1WPF_jp_AegxmDcGVIS3LySo7_aW47nz3slMqFuCe_hGMY6IujFZcHY50gk4Fd8rKaZai4V" />

The pros for this approach is that if the user already has logging instrumentation, they wouldn’t need to refactor any of their code. Also if engineers like working with logs, they get to keep them.

The downsides are the telemetry pipeline becomes more complex since we need to collect and store both logs and traces. Also, we’d need to combine the two data sources when it comes to querying and visualizing them in the UI, adding complexity to the present stage as well.

Given we want our users to not have to change their existing logging system to correlate logs with traces, we went with the option of inject trace context into logs instead of using span events.

To implement this, we could create a log appender library that extends popular logging libraries and injects the trace context. However, it turns out that OTel has instrumentation libraries for popular Node loggers. So as part of the auto-instrumentation code Hypha sets up for tracing, we also add instrumentation for logging libraries. These append the trace context directly to logs.

Now we can correlate logs with trace by the execution context!

## Deployment

Recall our fourth design goal from Hypha’s Architecture: Ready to Deploy. To meet this goal, Hypha should be platform agnostic, scalable, and automated as much as possible.

Early on, we decided to build Hypha’s backend using dockerized services and docker-compose. This makes Hypha easy to deploy across many environments and cloud platforms. Furthermore, it opens the door for using managed cloud container services, which provide elastic scaling capabilities. ADD MORE HERE

But first we needed to ensure that Hypha’s services were scalable to handle production level data. A focal point for Hypha’s pipeline was to ensure the Loki and Jaeger services within the backend are able to scale. Hypha configures both Jaeger and Loki with a read/write deployment configuration, thus separating out read and write service responsibilities to dedicated nodes. Compared to the default “all-in-one" deployment configuration, the “read/write" allows for higher availability, and flexibility to independently scale nodes as needed. For example, the Loki configuration

### Hypha Installation & Set Up

There are two steps to setting up Hypha:

- Deploy the Hypha backend infrastructure
- Download and setup the instrumentation and agent for each VM/service

**Deploy with Docker Compose**

Hypha provides easy deployment for the full Hypha backend within a Docker network.

First, download Docker Desktop (or Docker Compose for Linux) if not already on the machine.

Second, clone the `hypha-backend` repo.

Third, `cd` into the project root directory and run:

```
docker compose up
```

**Deploy with Docker ECS Integration**

Hypha also provides an automated deployment option for AWS ECS via Docker Compose.

First, download Docker Desktop (or Docker Compose for Linux), and clone the repo, if you haven’t already. You will also need to install the AWS CLI and configure it with your AWS credentials, if you haven’t done so.

Second, create a new Docker ECS context:

```
docker context create ecs <context-name>
```

Third, switch to your ECS context:

```
docker context use <context-name>
```

Fourth, from the project root directory, deploy to ECS by running:

```
docker compose up
```

Lastly, once the deployment processes complete, retrieve your gateway collector endpoint by running:

```
sh get-endpoints.sh
```

**Set Up Instrumentation & Agent**

Once you deploy the Hypha backend, you will need to set up the instrumentation and agent for each VM/service you wish to observe.

First, SSH into your VM.

Second, set up the tracing instrumentation. `cd` into your application directory and download the Hypha instrumentation file:

```
curl -O https://raw.githubusercontent.com/Team-Hypha/hypha-agent/main/tracing.js
```

Then restart your Node application to include the instrumentation:

```
node -r tracing.js <applicationfile>.js
```

Third, set up the agent.Download the install script:

```
curl -O https://raw.githubusercontent.com/Team-Hypha/hypha-agent/main/install.sh
```

Then execute it, passing in three arguments:

```
bash install.sh <service_name> <gateway_collector_endpoint> <log_file_path>
```

## Future Work

- Add instrumentation support for more languages (Go, Python, Ruby)
- Add support for additional logging libraries and formats
- Containerize Hypha Agent to support more deployment options such as Kubernetes
- Automate adding TLS certificates to Hypha backend

## References

1. (Distributed Tracing In Practice, page 237)
